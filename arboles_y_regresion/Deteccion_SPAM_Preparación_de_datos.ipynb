{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "ejMZxR3uYNBR"
      },
      "source": [
        "Se propone la construcción de un sistema de aprendizaje automático capaz de predecir si un correo determinado se corresponde con un correo de SPAM o no, para ello, se utilizará el siguiente conjunto de datos:\n",
        "2007 TREC Public Spam Corpus¶\n",
        "\n",
        "The corpus trec07p contains 75,419 messages:\n",
        "\n",
        "25220 ham\n",
        "50199 spam\n",
        "\n",
        "These messages constitute all the messages delivered to a particular server between these dates:\n",
        "\n",
        "Sun, 8 Apr 2007 13:07:21 -0400\n",
        "Fri, 6 Jul 2007 07:04:53 -0400\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tvDlVWBYNBS",
        "outputId": "8f055b5f-4e0a-4925-b76b-baec80f2a7a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "# Instalación de librerías externas\n",
        "!pip install scikit-learn\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_wAj9kgYNBU"
      },
      "source": [
        "### 1. Funciones complementarias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzzmB23AYNBW"
      },
      "source": [
        "En este caso práctico relacionado con la detección de correos electrónicos de SPAM, el conjunto de datos que disponemos esta formado por correos electrónicos, con sus correspondientes cabeceras y campos adicionales. Por lo tanto, requieren un preprocesamiento previo a que sean ingeridos por el algoritmo de Machine Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL0Ij57uYNBW"
      },
      "outputs": [],
      "source": [
        "# Esta clase facilita el preprocesamiento de correos electrónicos que poseen código HTML\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "class MLStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.strict = False\n",
        "        self.convert_charrefs = True\n",
        "        self.fed = []\n",
        "\n",
        "    def handle_data(self, d):\n",
        "        self.fed.append(d)\n",
        "\n",
        "    def get_data(self):\n",
        "        return ''.join(self.fed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSehOFQbYNBW"
      },
      "outputs": [],
      "source": [
        "# Esta función se encarga de elimar los tags HTML que se encuentren en el texto del correo electrónico\n",
        "def strip_tags(html):\n",
        "    s = MLStripper()\n",
        "    s.feed(html)\n",
        "    return s.get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aZ3cVFGYNBX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "08ccaad5-60e5-49a0-d139-38f7a90ad16c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Phrack World News'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Ejemplo de eliminación de los tags HTML de un texto\n",
        "t = '<tr><td align=\"left\"><a href=\"../../issues/51/16.html#article\">Phrack World News</a></td>'\n",
        "strip_tags(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSDMp6DAYNBX"
      },
      "source": [
        "Además de eliminar los posibles tags HTML que se encuentren en el correo electrónico, deben realizarse otras acciones de preprocesamiento para evitar que los mensajes contengan ruido innecesario. Entre ellas se encuentra la eliminación de los signos de puntuación, eliminación de posibles campos del correo electrónico que no son relevantes o eliminación de los afijos de una palabra manteniendo únicamente la raiz de la misma (Stemming). La clase que se muestra a continuación realiza estas transformaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iHy6HiGYNBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2779df9-a9a6-4905-defc-990abf305144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import email\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class Parser:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stemmer = nltk.PorterStemmer()\n",
        "        self.stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "        self.punctuation = list(string.punctuation)\n",
        "\n",
        "    def parse(self, email_path):\n",
        "        \"\"\"Parse an email.\"\"\"\n",
        "        with open(email_path, errors='ignore') as e:\n",
        "            msg = email.message_from_file(e)\n",
        "        return None if not msg else self.get_email_content(msg)\n",
        "\n",
        "    def get_email_content(self, msg):\n",
        "        \"\"\"Extract the email content.\"\"\"\n",
        "        subject = self.tokenize(msg['Subject']) if msg['Subject'] else []\n",
        "        body = self.get_email_body(msg.get_payload(),\n",
        "                                   msg.get_content_type())\n",
        "        content_type = msg.get_content_type()\n",
        "        # Returning the content of the email\n",
        "        return {\"subject\": subject,\n",
        "                \"body\": body,\n",
        "                \"content_type\": content_type}\n",
        "\n",
        "    def get_email_body(self, payload, content_type):\n",
        "        \"\"\"Extract the body of the email.\"\"\"\n",
        "        body = []\n",
        "        if type(payload) is str and content_type == 'text/plain':\n",
        "            return self.tokenize(payload)\n",
        "        elif type(payload) is str and content_type == 'text/html':\n",
        "            return self.tokenize(strip_tags(payload))\n",
        "        elif type(payload) is list:\n",
        "            for p in payload:\n",
        "                body += self.get_email_body(p.get_payload(),\n",
        "                                            p.get_content_type())\n",
        "        return body\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Transform a text string in tokens. Perform two main actions,\n",
        "        clean the punctuation symbols and do stemming of the text.\"\"\"\n",
        "        for c in self.punctuation:\n",
        "            text = text.replace(c, \"\")\n",
        "        text = text.replace(\"\\t\", \" \")\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        tokens = list(filter(None, text.split(\" \")))\n",
        "        # Stemming of the tokens\n",
        "        return [self.stemmer.stem(w) for w in tokens if w not in self.stopwords]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sMPmE5YYNBY"
      },
      "source": [
        "##### Lectura de un correo en formato raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSwrooR0YNBY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "a311a9a9-a588-4f85-8fd1-424be552ade8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'datasets/trec07p/data/inmail.1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8f089a616af6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minmail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/trec07p/data/inmail.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minmail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/trec07p/data/inmail.1'"
          ]
        }
      ],
      "source": [
        "inmail = open(\"datasets/trec07p/data/inmail.1\").read()\n",
        "print(inmail)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs211pWFYNBY"
      },
      "source": [
        "##### Parsing del correo electrónico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5Q_jzpNYNBY"
      },
      "outputs": [],
      "source": [
        "p = Parser()\n",
        "p.parse(\"datasets/trec07p/data/inmail.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgd2fv3QYNBY"
      },
      "source": [
        "##### Lectura del índice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjqik_VVYNBZ"
      },
      "source": [
        "Estas funciones complementarias se encargan cargar en memoria la ruta de cada correo electrónico y su etiqueta correspondiente {spam, ham}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjTRUSgvYNBZ"
      },
      "outputs": [],
      "source": [
        "index = open(\"datasets/trec07p/full/index\").readlines()\n",
        "index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwCRmpjMYNBZ"
      },
      "outputs": [],
      "source": [
        "def parse_index(path_to_index, n_elements):\n",
        "    ret_indexes = []\n",
        "    index = open(path_to_index).readlines()\n",
        "    for i in range(n_elements):\n",
        "        mail = index[i].split(\" ../\")\n",
        "        label = mail[0]\n",
        "        path = mail[1][:-1]\n",
        "        path_mail = path.split(\"/\")[-1]\n",
        "        ret_indexes.append({\"label\":label, \"email_path\":os.path.join(DATASET_PATH, os.path.join(\"data\", path_mail))})\n",
        "    return ret_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwuwUhXPYNBZ"
      },
      "outputs": [],
      "source": [
        "def parse_email(index):\n",
        "    p = Parser()\n",
        "    pmail = p.parse(index[\"email_path\"])\n",
        "    return pmail, index[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAGjkckFYNBZ"
      },
      "outputs": [],
      "source": [
        "indexes = parse_index(\"datasets/trec07p/full/index\", 10)\n",
        "indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovZl__EsYNBZ"
      },
      "source": [
        "### 2. Preprocesamiento de los datos del conjunto de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jOAiHj2YNBZ"
      },
      "source": [
        "Con las funciones presentadas anteriormente se permite la lectura de los correos electrónicos de manera programática y el procesamiento de los mismos para eliminar aquellos componentes que no resultan de utilidad para la detección de correos de SPAM. Sin embargo, cada uno de los correos sigue estando representado por un diccionario de Python con una serie de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQo6qsogYNBZ"
      },
      "outputs": [],
      "source": [
        "# Cargamos el índice y las etiquetas en memoria\n",
        "index = parse_index(\"datasets/trec07p/full/index\", 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF_IeEzMYNBa"
      },
      "outputs": [],
      "source": [
        "# Leemos el primer correo\n",
        "import os\n",
        "\n",
        "open(index[0][\"email_path\"]).read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_eLOylcYNBa"
      },
      "outputs": [],
      "source": [
        "# Parseamos el primer correo\n",
        "mail, label = parse_email(index[0])\n",
        "print(\"El correo es:\", label)\n",
        "print(mail)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09pPqvscYNBa"
      },
      "source": [
        "El algoritmo de Regresión Logística no es capaz de ingerir texto como parte del conjunto de datos. Por lo tanto, deben aplicarse una serie de funciones adicionales que transformen el texto de los correos electrónicos parseados en una representación numérica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vqjgh01YNBa"
      },
      "source": [
        "##### Aplicación de CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsWMA8tDYNBa"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Preapración del email en una cadena de texto\n",
        "prep_email = [\" \".join(mail['subject']) + \" \".join(mail['body'])]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit(prep_email)\n",
        "\n",
        "print(\"Email:\", prep_email, \"\\n\")\n",
        "print(\"Características de entrada:\", vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE3NCSdZYNBa"
      },
      "outputs": [],
      "source": [
        "X = vectorizer.transform(prep_email)\n",
        "print(\"\\nValues:\\n\", X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx_uWPWKYNBa"
      },
      "source": [
        "##### Aplicación de OneHotEncoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhvP70JiYNBa"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "prep_email = [[w] for w in mail['subject'] + mail['body']]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "X = enc.fit_transform(prep_email)\n",
        "\n",
        "print(\"Features:\\n\", enc.get_feature_names_out())\n",
        "print(\"\\nValues:\\n\", X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8khjngcKYNBb"
      },
      "source": [
        "##### Funciones auxiliares para preprocesamiento del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPSXpPRxYNBb"
      },
      "outputs": [],
      "source": [
        "def create_prep_dataset(index_path, n_elements):\n",
        "    X = []\n",
        "    y = []\n",
        "    indexes = parse_index(index_path, n_elements)\n",
        "    for i in range(n_elements):\n",
        "        print(\"\\rParsing email: {0}\".format(i+1), end='')\n",
        "        try:\n",
        "            mail, label = parse_email(indexes[i])\n",
        "            X.append(\" \".join(mail['subject']) + \" \".join(mail['body']))\n",
        "            y.append(label)\n",
        "        except:\n",
        "            pass\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDGbg_GZYNBb"
      },
      "source": [
        "### 3. Entrenamiento del algoritmo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNuklW0sYNBb"
      },
      "source": [
        "### 4. Predicción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdp5kVUCYNBb"
      },
      "source": [
        "### 5. Aumentando el conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC3zFKU8YNBb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}