{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnuK8W7T2roQ",
        "outputId": "fcc278be-069a-4035-f02a-4a8a250b171a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m194.6/235.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n",
            "Collecting LSTM\n",
            "  Downloading lstm-0.1.0-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Installing collected packages: LSTM\n",
            "Successfully installed LSTM-0.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud promedio de las frases: 68.61963819470347\n",
            "Longitud mediana de las frases: 36.0\n",
            "Número de secuencias: 474818\n",
            "Epoch 1/10\n",
            "7420/7420 [==============================] - 63s 8ms/step - loss: 2.2588\n",
            "Epoch 2/10\n",
            "7420/7420 [==============================] - 60s 8ms/step - loss: 1.9801\n",
            "Epoch 3/10\n",
            "7420/7420 [==============================] - 58s 8ms/step - loss: 1.8959\n",
            "Epoch 4/10\n",
            "7420/7420 [==============================] - 58s 8ms/step - loss: 1.8467\n",
            "Epoch 5/10\n",
            "7420/7420 [==============================] - 60s 8ms/step - loss: 1.8137\n",
            "Epoch 6/10\n",
            "7420/7420 [==============================] - 57s 8ms/step - loss: 1.7877\n",
            "Epoch 7/10\n",
            "7420/7420 [==============================] - 58s 8ms/step - loss: 1.7671\n",
            "Epoch 8/10\n",
            "7420/7420 [==============================] - 57s 8ms/step - loss: 1.7505\n",
            "Epoch 9/10\n",
            "7420/7420 [==============================] - 57s 8ms/step - loss: 1.7380\n",
            "Epoch 10/10\n",
            "7420/7420 [==============================] - 57s 8ms/step - loss: 1.7267\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode\n",
        "!pip install LSTM nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import numpy as np\n",
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import unidecode\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Paso 1: Obtener y preprocesar el texto\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "path = \"pg4300.txt\"\n",
        "with open(path) as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Preprocesar texto (simplificado, considerar limpiar encabezados y pies de página del sitio)\n",
        "text = text.lower()  # Convertir a minúsculas\n",
        "# Asumiendo que 'text' es tu texto completo\n",
        "sentences = text.split('.')  # Esto divide el texto en frases basándose en el punto.\n",
        "\n",
        "# Crear un DataFrame con las frases\n",
        "df_sentences = pd.DataFrame(sentences, columns=['sentence'])\n",
        "\n",
        "# Eliminar espacios en blanco y filtrar frases vacías\n",
        "df_sentences['sentence'] = df_sentences['sentence'].str.strip()\n",
        "df_sentences = df_sentences[df_sentences['sentence'] != '']\n",
        "#longitud en caracteres\n",
        "df_sentences['length'] = df_sentences['sentence'].str.len()\n",
        "average_length = df_sentences['length'].mean()\n",
        "median_length = df_sentences['length'].median()\n",
        "print(\"Longitud promedio de las frases:\", average_length)\n",
        "print(\"Longitud mediana de las frases:\", median_length)\n",
        "\n",
        "# Eliminar caracteres especiales y números\n",
        "text = re.sub(r'[^a-zA-ZáéíóúñÁÉÍÓÚÑ ]', '', text)\n",
        "# Normalizar acentos\n",
        "text = unidecode.unidecode(text)\n",
        "\n",
        "# Tokenización\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Eliminar palabras de parada\n",
        "filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
        "\n",
        "# Reconstruir el texto\n",
        "cleaned_text = ' '.join(filtered_words)\n",
        "\n",
        "chars = sorted(list(set(cleaned_text)))  # Listar caracteres únicos\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))  # Crear diccionario de caracteres a índices\n",
        "\n",
        "# Crear secuencias de caracteres y sus próximos caracteres\n",
        "maxlen = 50\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('Número de secuencias:', len(sentences))\n",
        "\n",
        "# Vectorizar las secuencias de entrada y etiquetas\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_to_int[char]] = 1\n",
        "    y[i, char_to_int[next_chars[i]]] = 1\n",
        "\n",
        "# Paso 2: Construir el modelo RNN\n",
        "model = Sequential()\n",
        "# Primera capa LSTM con return_sequences=True para apilar otra capa LSTM después\n",
        "model.add(LSTM(128, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
        "# Opcionalmente, agrega Dropout para regularización\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "# Segunda capa LSTM\n",
        "model.add(LSTM(128))\n",
        "# Otra capa de Dropout\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "# Capa de salida\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSprop')\n",
        "\n",
        "# Paso 3: Entrenar el modelo\n",
        "model.fit(x, y, batch_size=64, epochs=10)  # Ajusta el número de épocas según sea necesario\n",
        "model.save('ulisses_0.2_token.keras')"
      ]
    }
  ]
}