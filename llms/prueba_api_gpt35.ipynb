{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"analisis\": {\n",
      "    \"ideas\": [\n",
      "      \"La diversidad arquitectónica y cultural se entrelaza en la ciudad de Los Ángeles.\",\n",
      "      \"Existen encantadoras casas de estilo español en barrios históricos.\",\n",
      "      \"También se encuentran modernas mansiones en las colinas de la ciudad.\",\n",
      "      \"Las avenidas son animadas y bulliciosas, mientras que los vecindarios son tranquilos.\",\n",
      "      \"Los jardines y patios son exuberantes y soleados, invitando al estilo de vida al aire libre.\",\n",
      "      \"La influencia cultural se refleja en cada detalle de la arquitectura de Los Ángeles.\"\n",
      "    ]\n",
      "  },\n",
      "  \"conclusión\": \"Los Ángeles es una ciudad con una amplia diversidad arquitectónica y cultural, que se manifiesta en sus casas de estilo español, mansiones modernas, animadas avenidas, tranquilos vecindarios y jardines exuberantes. La influencia cultural está presente en cada detalle de la arquitectura, convirtiendo a Los Ángeles en un lienzo habitable y siempre cambiante.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Configure the OpenAI engine\n",
    "engine = \"gpt-3.5-turbo\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create a completion using the OpenAI API\n",
    "def get_completion(prompt):\n",
    "\tcompletion = client.chat.completions.create(\n",
    "\t\tmodel=engine,\n",
    "\t\tmessages=[\n",
    "\t\t\t{\"role\": \"system\", \"content\": \"Eres un asistente, que realizas resúmenes concisos y proporcionas la ideas principales de un texto. \\\n",
    "\t         Estas ideas me las proporcionarás en una lista de puntos numerados, y añadirás una conclusión global, lo \\\n",
    "\t         el formato de salida debe ser de tipo .json, las ideas en una propiedad llamada 'analisis' con un array llamado 'ideas' y las conclusiones las añades \\\n",
    "\t          a una propiedad conslusión. Finalmete añade como metadatos alguna version y licencia\" },\n",
    "\t\t\t{\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "\t\t]\n",
    "\t)\n",
    "\treturn(completion)\n",
    "\n",
    "text = f\"\"\"En el corazón bullicioso de la ciudad de Los Ángeles, donde las palmas se alzan en el horizonte y el sol acaricia las fachadas de las viviendas, la diversidad arquitectónica y cultural se entrelaza en una sinfonía visual. Desde las encantadoras casas de estilo español con sus tejas rojas en barrios históricos hasta las modernas mansiones que se asoman en las colinas, cada rincón cuenta su propia historia.\n",
    "\n",
    "Las avenidas animadas se llenan con el zumbido de la vida urbana, mientras que los tranquilos \\\n",
    "vecindarios ofrecen refugio y serenidad. Jardines exuberantes y patios soleados se convierten en \\\n",
    "extensiones naturales de los hogares, invitando al estilo de vida al aire libre característico de la región.\\\n",
    "En este mosaico de viviendas, la influencia cultural se refleja en cada detalle, creando una paleta arquitectónica\\\n",
    "única que hace de Los Ángeles un lienzo habitable y siempre cambiante.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Resume el texto delimitado por triples acentos graves \\\n",
    "en una sola frase.\n",
    "'''{text}'''\n",
    "\"\"\"\n",
    "# Print the generated completion\n",
    "response = get_completion(prompt)\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta para receta:\n",
      "No se proporcionan instrucciones.\n"
     ]
    }
   ],
   "source": [
    "def get_completion(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=engine,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "        ]\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "receta = f\"\"\"\n",
    "Mezcla en un tazón apto para microondas 4 cucharadas de azúcar, 2 cucharadas de harina, 2 cucharadas de cacao puro en polvo, una \n",
    "pizca de sal y 1/4 de cucharadita de levadura. Añade un huevo batido y 2 cucharadas de aceite de oliva. Remueve bien hasta \n",
    "obtener una masa uniforme. Si deseas, puedes agregar chispas de chocolate o nueces al gusto. Una vez mezclado, aplana la \n",
    "superficie con una cuchara. Cocina en el microondas a máxima potencia durante aproximadamente 1 minuto. Mantén un ojo en él \n",
    "microondas para evitar que se reseque. Deja enfriar un poco antes de comer, ya que estará muy caliente.\n",
    "\"\"\"\n",
    "receta2 = \"\"\"\n",
    "En el tranquilo rincón de la ciudad, donde las luces de la calle bailan suavemente en la noche, se encuentra un café acogedor con sillas cómodas y el aroma reconfortante del café recién hecho. Los clientes, inmersos en sus conversaciones y libros, disfrutan de la calidez del lugar mientras las horas transcurren sin prisa. La barista, con una sonrisa amable, prepara con destreza una variedad de bebidas, desde el clásico espresso hasta creativas mezclas con nombres intrigantes. En este espacio, el tiempo se desliza suavemente, creando un refugio para aquellos que buscan un momento de tranquilidad en medio del ajetreo diario.\n",
    "\"\"\" #sin instrucciones\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Te proporcionaré un texto en comillas triples\n",
    "Si contiene una secuencia de instrucciones, \\\n",
    "re-escribe esas instrucciones en el siguiente formato:\n",
    "Paso 1 -\n",
    "...\n",
    "Paso 2 -\n",
    "…\n",
    "…\n",
    "Paso N -\n",
    "…\n",
    "Si el texto no contiene una secuencia de instrucciones, \\\n",
    "entonces simplemente escribe \\\"No se proporcionan instrucciones.\\\"\n",
    "\\\"\\\"\\\"{receta2}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(\"Respuesta para receta:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# few shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta para texto:\n",
      "<profesor> La resiliencia, colega, es la capacidad de enfrentar los golpes de la vida sin dejarse caer. Es como decirle al destino \"ponte en fila, que aquí sigo de pie\". En el día a día, cuando las cosas se ponen duras, la resiliencia es esa onda que te ayuda a levantarte una y otra vez, sin rendirte. No es ignorar los problemas, es enfrentarlos de frente y sacar fuerzas de donde sea para seguir adelante. La resiliencia es amor propio, tío.\n"
     ]
    }
   ],
   "source": [
    "texto = f\"\"\"\n",
    "<alumno> Enséñame sobre paciencia\n",
    "<profesor> La paciencia, bro, es aguantarse la movida sin flipar. Es como,\n",
    "colega, no pierdas la cabeza\". \\\n",
    "En el rollo diario, cuando todo va fatal, la paciencia es esa onda que te mantiene\n",
    "chido, sin volverte loco. No es solo \\\n",
    "esperar, es dominar el juego, ser el crack que controla el rollo, no el que se va de\n",
    "la olla por cualquier cosa. La paciencia es poder, tío.\n",
    "<alumno> Enséñame sobre resiliencia\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Te proporcionaré un texto en comillas triples\n",
    "Tu tarea es contestar en un estilo consecuente:\n",
    "\\\"\\\"\\\"{texto}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Respuesta para texto:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Técnica time to think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-YXctQ***************************************JUch. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mEn un pintoresco rincón del bosque, los amigos Carlos y Carolina decidieron explorar un sendero rodeado de árboles \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mcentenarios. Mientras caminaban, descubrieron un arroyo cristalino que serpenteba entre las piedras. Decidieron detenerse \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124my disfrutar de la serenidad del lugar. Carlos encontró una piedra peculiar y se la llevó como recuerdo. A medida que el sol \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mse ocultaba, regresaron a casa con corazones llenos de gratitud por la belleza de la naturaleza.\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m prompt_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mRealiza las siguientes acciones:\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m1 - Resume el siguiente texto delimitado por triples comillas invertidas en una sola oración.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;132;01m{text}\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m response \u001b[38;5;241m=\u001b[39m get_completion(prompt)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRespuesta para texto:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m, in \u001b[0;36mget_completion\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_completion\u001b[39m(prompt):\n\u001b[0;32m----> 9\u001b[0m \tcompletion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     10\u001b[0m \t\tmodel\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m     11\u001b[0m \t\tmessages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     12\u001b[0m \t\t\t{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEres un asistente, que realizas resúmenes concisos y proporcionas la ideas principales de un texto. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m\t         Estas ideas me las proporcionarás en una lista de puntos numerados, y añadirás una conclusión global, lo \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m\t         el formato de salida debe ser de tipo .json, las ideas en una propiedad llamada \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalisis\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m con un array llamado \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mideas\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m y las conclusiones las añades \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m\t          a una propiedad conslusión. Finalmete añade como metadatos alguna version y licencia\u001b[39m\u001b[38;5;124m\"\u001b[39m },\n\u001b[1;32m     16\u001b[0m \t\t\t{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     17\u001b[0m \t\t]\n\u001b[1;32m     18\u001b[0m \t)\n\u001b[1;32m     19\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m(completion)\n",
      "File \u001b[0;32m~/anaconda3/envs/base_ml/lib/python3.12/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/base_ml/lib/python3.12/site-packages/openai/resources/chat/completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    658\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    661\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    662\u001b[0m             {\n\u001b[1;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    683\u001b[0m             },\n\u001b[1;32m    684\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    685\u001b[0m         ),\n\u001b[1;32m    686\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    687\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    688\u001b[0m         ),\n\u001b[1;32m    689\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    690\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    692\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/base_ml/lib/python3.12/site-packages/openai/_base_client.py:1180\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1177\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1178\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1179\u001b[0m     )\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/envs/base_ml/lib/python3.12/site-packages/openai/_base_client.py:869\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    862\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    868\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    870\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    871\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    872\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    873\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    874\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    875\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/base_ml/lib/python3.12/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    959\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    963\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    964\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    968\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-YXctQ***************************************JUch. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "En un pintoresco rincón del bosque, los amigos Carlos y Carolina decidieron explorar un sendero rodeado de árboles \\\n",
    "centenarios. Mientras caminaban, descubrieron un arroyo cristalino que serpenteba entre las piedras. Decidieron detenerse \\\n",
    "y disfrutar de la serenidad del lugar. Carlos encontró una piedra peculiar y se la llevó como recuerdo. A medida que el sol \\\n",
    "se ocultaba, regresaron a casa con corazones llenos de gratitud por la belleza de la naturaleza.\n",
    "\"\"\"\n",
    "prompt_1 = \"\"\"\n",
    "Realiza las siguientes acciones:\n",
    "1 - Resume el siguiente texto delimitado por triples comillas invertidas en una sola oración.\n",
    "2 - Traduce el resumen al francés.\n",
    "3 - Enumera cada nombre en el resumen francés.\n",
    "4 - Muestra un objeto JSON que contenga las siguientes claves: resumen_frances, num_nombres.\n",
    "Separa tus respuestas con saltos de línea.\n",
    "Texto:\n",
    "{text}\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Respuesta para texto:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, permíteme explicarte cómo funcionan los firewalls de una manera sencilla.\n",
      "\n",
      "Imagina que tienes una casa y quieres protegerla de posibles intrusos. Lo que haces es instalar una puerta con candado para asegurarte de que solo las personas autorizadas puedan entrar.\n",
      "\n",
      "En el mundo de la informática, un firewall hace algo similar. Es una barrera de seguridad que se coloca entre tu computadora (o una red de computadoras) y la conexión a internet. Su función principal es proteger tu sistema de posibles amenazas y ataques provenientes de internet.\n",
      "\n",
      "El firewall examina todo el tráfico de datos que entra y sale de tu red y toma decisiones sobre si permitir o bloquear esos datos según un conjunto de reglas predefinidas. Esas reglas se basan en diferentes criterios, como el tipo de conexión, la dirección IP, el puerto utilizado y otros elementos.\n",
      "\n",
      "Puedes imaginar al firewall como una especie de filtro de seguridad que filtra el tráfico de datos y solo permite que los datos seguros y autorizados pasen y entren en tu red. Como resultado, se evita que posibles amenazas o intrusos ingresen a tu sistema.\n",
      "\n",
      "Existen diferentes tipos de firewalls, como los firewalls de red, que se sitúan entre tu red local y la conexión a internet, y los firewalls de software, que se instalan en tu computadora. Ambos cumplen la misma función de protección, pero operan en diferentes niveles.\n",
      "\n",
      "En resumen, los firewalls funcionan como una especie de \"puerta con candado\" que protege tus dispositivos y redes de posibles amenazas externas.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Configurar el motor de OpenAI\n",
    "engine = \"gpt-3.5-turbo\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_completion(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=engine,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Eres un experto en seguridad informática con más de 10 años de experiencia, \"\n",
    "                                          \"tu conocimiento se extiende por múltiples aspectos de la seguridad informática y física y dispones de \"\n",
    "                                          \"numerosas certificaciones en este campo. Además, eres experto en administración de sistemas.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "        ]\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "prompt = (\n",
    "    f\"Explica cómo funcionan los firewalls en términos sencillos, como si estuvieras enseñando a un estudiante de primer año en \"\n",
    "    \"informática.\"\n",
    ")\n",
    "respuesta = get_completion(prompt)\n",
    "print(respuesta.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solución:\n",
      "\n",
      "Supongamos que x es el tamaño de la instalación en pies cuadrados.\n",
      "\n",
      "Costos:\n",
      "- Costo del terreno: $100 x\n",
      "- Costo de los paneles solares: $250 x\n",
      "- Costo de mantenimiento: $100,000 + $10 x\n",
      "\n",
      "Para calcular el costo total, sumamos todos los costos:\n",
      "\n",
      "Costo total = Costo del terreno + Costo de los paneles solares + Costo de mantenimiento\n",
      "\n",
      "Costo total = $100 x + $250 x + ($100,000 + $10 x)\n",
      "\n",
      "Simplificando la expresión:\n",
      "\n",
      "Costo total = $350 x + $100,000 + $10 x\n",
      "\n",
      "Costo total = $360 x + $100,000\n",
      "\n",
      "La solución del alumno es incorrecta. El estudiante olvidó incluir el término \"$10 x\" en el cálculo del costo total.\n",
      "\n",
      "La ecuación correcta para el costo total es:\n",
      "\n",
      "Costo total = $360 x + $100,000\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Tu tarea es determinar si la solución del estudiante es correcta o no.\n",
    "Para resolver el problema, realiza lo siguiente:\n",
    "- Primero, elabora tu propia solución al problema, incluyendo el total final.\n",
    "- Luego, compara tu solución con la solución del estudiante y asegúrate de si la solución del estudiante es correcta o no.\n",
    "No decidas si la solución del estudiante es correcta hasta que hayas resuelto el problema tú mismo.\n",
    "Utiliza el siguiente formato:\n",
    "Pregunta:\n",
    "\n",
    "```\n",
    "Estoy construyendo una instalación de energía solar y necesito ayuda para calcular los aspectos financieros.\n",
    "El terreno cuesta $100 / pie cuadrado\n",
    "Puedo comprar paneles solares por $250 / pie cuadrado\n",
    "Negocié un contrato de mantenimiento que me costará un fijo de $100k por año, y un adicional de $10 / pie cuadrado\n",
    "¿Cuál es el costo total para el primer año de operaciones en función del número de pies cuadrados?\n",
    "```\n",
    "Solución del alumno:\n",
    "```\n",
    "Supongamos que x es el tamaño de la instalación en pies cuadrados.\n",
    "Costos:\n",
    "Costo del terreno: 100x\n",
    "Costo de los paneles solares: 250x\n",
    "Costo de mantenimiento: 100,000 + 100x\n",
    "Costo total: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cambio de engine a gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La solución del estudiante casi es correcta, pero hay un pequeño error en el cálculo del costo de mantenimiento. El estudiante ha usado incorrectamente \"100x\" en lugar de \"10x\" para el costo adicional por pie cuadrado en la parte del mantenimiento. La solución correcta sería:\n",
      "\n",
      "Costo del terreno: 100x\n",
      "Costo de los paneles solares: 250x\n",
      "Costo de mantenimiento: 100,000 + 10x\n",
      "\n",
      "Por lo tanto, el costo total para el primer año de operaciones en función del número de pies cuadrados (x) sería:\n",
      "\n",
      "Costo total = Costo del terreno + Costo de los paneles solares + Costo de mantenimiento\n",
      "Costo total = 100x + 250x + (100,000 + 10x)\n",
      "Costo total = 100x + 250x + 100,000 + 10x\n",
      "Costo total = (100 + 250 + 10)x + 100,000\n",
      "Costo total = 360x + 100,000\n",
      "\n",
      "Así que la fórmula correcta para el costo total es 360x + 100,000, donde x es el tamaño de la instalación en pies cuadrados.\n"
     ]
    }
   ],
   "source": [
    "engine = \"gpt-4-1106-preview\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "prompt = \"\"\"\n",
    "Tu tarea es determinar si la solución del estudiante es correcta o no.\n",
    "Pregunta:\n",
    "\n",
    "```\n",
    "Estoy construyendo una instalación de energía solar y necesito ayuda para calcular los aspectos financieros.\n",
    "El terreno cuesta $100 / pie cuadrado\n",
    "Puedo comprar paneles solares por $250 / pie cuadrado\n",
    "Negocié un contrato de mantenimiento que me costará un fijo de $100k por año, y un adicional de $10 / pie cuadrado\n",
    "¿Cuál es el costo total para el primer año de operaciones en función del número de pies cuadrados?\n",
    "```\n",
    "Solución del alumno:\n",
    "```\n",
    "Supongamos que x es el tamaño de la instalación en pies cuadrados.\n",
    "Costos:\n",
    "Costo del terreno: 100x\n",
    "Costo de los paneles solares: 250x\n",
    "Costo de mantenimiento: 100,000 + 100x\n",
    "Costo total: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, temperature=0.7):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=engine,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alucinación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La técnica de encriptación cuántica basada en superposición de estados es una herramienta de seguridad que se basa en las propiedades de la mecánica cuántica para asegurar la transmisión de información de manera segura. A diferencia de los métodos clásicos de encriptación, que se basan en algoritmos complejos, la encriptación cuántica utiliza principios cuánticos como la superposición de estados y la intrincación para proteger la información.\n",
      "\n",
      "En la encriptación cuántica, los mensajes se codifican en estados cuánticos, como los qubits, que representan la unidad básica de información cuántica. Estos estados pueden estar en una superposición de múltiples valores al mismo tiempo, lo que permite una codificación más robusta y difícil de hackear.\n",
      "\n",
      "En términos de aplicaciones en la fabricación de yogur, puede parecer extraño, pero la encriptación cuántica podría tener beneficios en la cadena de suministro del yogur. Por ejemplo, si se utiliza en la comunicación entre diferentes etapas de producción y transporte, puede garantizar la seguridad de los datos relacionados con la temperatura, la calidad y otros parámetros críticos para la fermentación del yogur.\n",
      "\n",
      "Además, la encriptación cuántica también podría ser útil para proteger información confidencial relacionada con recetas de yogur o procesos de fermentación patentados. Al utilizar técnicas de encriptación cuántica, se podría garantizar que esta información solo pueda ser accedida por las partes autorizadas, evitando el riesgo de robos de propiedad intelectual o secretos comerciales.\n",
      "\n",
      "En resumen, aunque a primera vista pueda parecer que la encriptación cuántica no tiene aplicaciones directas en la fermentación de yogur, puede desempeñar un papel importante en la seguridad de la cadena de suministro y en la protección de información confidencial en la industria del yogur.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure OpenAI engine\n",
    "engine = \"gpt-3.5-turbo\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_completion(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=engine,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "        ]\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "prompt = \"\"\"\n",
    "Explícame la técnica de encriptación cuántica basada en superposición de estados y su aplicación en la fabricación de yogurt. Tiene aplicaciones en la \\\n",
    "    fermentación de yogur aunque a primera vista no lo parezca\"\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La teoría de la evolución postulada por Charles Darwin en el siglo XIX propone que los organismos vivos evolucionan a lo largo del tiempo a través de un proceso de selección natural. Esta teoría explica que los organismos con características más favorables para sobrevivir y reproducirse tienen una mayor probabilidad de transmitir esas características a la siguiente generación.\n",
      "\n",
      "Ahora bien, en cuanto a la evolución del \"chimpafante de Aragón\", debemos aclarar que no existe tal criatura. Parece ser un neologismo utilizado como ejemplo ficticio. Sin embargo, podemos utilizar la teoría de la evolución para explicar cómo podría haber ocurrido tal evolución.\n",
      "\n",
      "Si consideramos el entorno de Aragón, una región en España, y asumimos que había una población de chimpancés y elefantes, podríamos postular una serie de cambios en el ambiente que podrían llevar a la adaptación de estos organismos y, eventualmente, al desarrollo de un ser híbrido entre ambos.\n",
      "\n",
      "Por ejemplo, podría haber ocurrido una sequía prolongada en la región, lo cual habría llevado a una escasez de alimentos y recursos para los chimpancés y elefantes. En esta situación, solo aquellos individuos con adaptaciones favorables para sobrevivir en condiciones de escasez habrían tenido mayores posibilidades de reproducirse.\n",
      "\n",
      "Supongamos que algunos chimpancés desarrollaron características como un mayor tamaño corporal, una trompa más alargada y una dieta más variada para aprovechar los distintos recursos alimentarios. Estos individuos híbridos tendrían una ventaja adaptativa, ya que podrían acceder tanto a los alimentos que los chimpancés como a los elefantes consumen. A lo largo de las generaciones, estos individuos híbridos habrían sido seleccionados positivamente y pasarían sus características adaptativas a su descendencia.\n",
      "\n",
      "En un periodo de tiempo prolongado, podríamos imaginar que la población de estos seres híbridos se habría establecido en la región de Aragón y habría evolucionado aún más, desarrollando características específicas para su entorno. Sin embargo, es importante recalcar que esto es meramente una especulación basada en supuestos y no hay evidencia científica de la existencia de tal criatura.\n",
      "\n",
      "En resumen, la teoría de la evolución de Darwin proporciona una explicación sobre cómo los organismos pueden cambiar y adaptarse a través de selección natural en respuesta a cambios en su entorno. Sin embargo, en el caso del \"chimpafante de Aragón\", se trata de un ejemplo ficticio para ilustrar el concepto.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Explícame la teoría de la evolución y, basandote en ella, explica la evolución del chimpafante de Aragón\"\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:79: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:79: SyntaxWarning: invalid escape sequence '\\T'\n",
      "/var/folders/yc/q9_6ht1x43s552t2rfm6pw1c0000gn/T/ipykernel_53501/1318495502.py:79: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descubre el curso de Python de Sinensia IT Solutions, diseñado para principiantes y profesionales en desarrollo de software. Aprende los conceptos básicos de programación, tipos de datos, control de flujo y más. Este curso te proporcionará una base sólida en Python, permitiéndote escribir programas simples, modificar código existente y abordar desafíos de programación. ¡Inicia tu viaje en la ciencia de datos y el desarrollo profesional con Python! Duración: 30 horas. Código: OPSPY02.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Configurar el motor de OpenAI\n",
    "engine = \"gpt-3.5-turbo\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_completion(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=engine,\n",
    "        messages=messages,\n",
    "        temperature=0.3,  # Esta es la creatividad del modelo\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "curso = f\"\"\"\n",
    "1. Introducción\n",
    "-Entorno de desarrollo\n",
    "-Intérprete\n",
    "-Hola Mundo\n",
    "-Python2 vs Python3\n",
    "2. Sintaxis de Python\n",
    "-Tipos de datos básicos\n",
    "-Tipos compuestos, secuencias\n",
    "-Operadores\n",
    "-Condicionales\n",
    "-Control de flujo\n",
    "-Bucles\n",
    "-Funciones\n",
    "SINENSIA IT SOLUTIONS\n",
    "3. Strings y salida por consola\n",
    "-Funciones básicas de strings\n",
    "4. Secuencias, listas y diccionarios\n",
    "-Operaciones básicas\n",
    "-Búsqueda, ordenación\n",
    "-Iteradores\n",
    "-Generadores\n",
    "-Lambdas\n",
    "5. Modularidad\n",
    "-Módulos standard\n",
    "-Nuestros propios módulos\n",
    "-Packages\n",
    "6. Clases\n",
    "-Programación Orientada a Objetos\n",
    "-Herencia, Encapsulación, polimorfismo\n",
    "-Programación de clases\n",
    "7. Ficheros\n",
    "-Lectura y escritura en ficheros\n",
    "-Formatos\n",
    "8. Excepciones\n",
    "-Tratamiento de errores\n",
    "-Try/except/else/finally\n",
    "-Programación de excepciones propias\n",
    "SINENSIA IT SOLUTIONS\n",
    "Audiencia:\n",
    "Curso dirigido a principiantes en programación y profesionales que \\\n",
    "deseen adquirir habilidades en el desarrollo de software utilizando Python.\n",
    "\\También puede ser interesante para aquellos que deseen comenzar su viaje \\\n",
    "en la ciencia de datos, ya que Python es una herramienta esencial en este campo.\n",
    "Prerrequisitos:\n",
    "Los alumnos necesitarán tener conocimientos básicos de programación.\n",
    "Objetivos:\n",
    "Este curso tiene como objetivo proporcionar a los participantes una base sólida \\\n",
    "en programación utilizando Python como lenguaje principal.\n",
    "Al finalizar el curso, los participantes estarán capacitados para escribir \\\n",
    "programas simples utilizando los conceptos y técnicas aprendidos, comprender \\\n",
    "y modificar código existente, así como abordar problemas y desafíos de \\\n",
    "programación utilizando Python.\n",
    "Código: OPSPY02\n",
    "Metodología: ILT\n",
    "Duración: 30 Horas\n",
    "Habilidades: Desarrollo de Aplicaciones\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Tu tarea es ayudar a un equipo de marketing a \\\n",
    "crear una descripción para un sitio web de ventas \\\n",
    "de un producto, basándote en una ficha técnica.\n",
    "Escribe una descripción del producto con base en la \\\n",
    "información proporcionada en la ficha de curso\n",
    "del curso delimitado por triple acento grave\n",
    "No incluyas datos que ya aparecen en la ficha como la\n",
    "duración o el código.\n",
    "Usa como máximo 80 palabras. Enfócate en los \\\n",
    "conceptos relativos a el futuro que tienen los conocimientos,\\\n",
    "la utilidad y el desarrollo profesional.\n",
    "Ficha de curso: ```{curso}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- La primera valoración destaca la facilidad de uso y el servicio al cliente excepcional del fabricante.\n",
      "- La segunda valoración resalta la llegada del producto en perfectas condiciones y antes de lo esperado, así como su eficiencia y diseño elegante.\n",
      "- En la tercera valoración, se destaca la calidad y tecnología innovadora del producto, así como su durabilidad y resistencia.\n",
      "- En la cuarta valoración, se menciona la decepción con el producto debido a problemas de funcionamiento, un servicio al cliente poco útil y la falta de cumplimiento de las promesas del fabricante.\n",
      "- La quinta valoración expresa insatisfacción con el producto debido a un defecto de fábrica, problemas para obtener un reemplazo, bajo rendimiento y baja relación calidad-precio.\n",
      "\n",
      "En general, las valoraciones positivas destacan aspectos como la facilidad de uso, el servicio al cliente excepcional, la calidad, eficiencia y diseño del producto. Las valoraciones negativas, por otro lado, mencionan problemas de funcionamiento, falta de cumplimiento de las promesas del fabricante, servicio al cliente deficiente y problemas en el proceso de reemplazo, así como un rendimiento mediocre y una baja relación calidad-precio.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Configurar el motor de OpenAI\n",
    "engine = \"gpt-3.5-turbo\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_completion(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=engine,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente, que realizas resúmenes concisos y proporcionas la ideas principales de un texto.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "        ]\n",
    "    )\n",
    "    return completion\n",
    "review1 = \"\"\"\n",
    "He comprado recientemente este producto y debo decir que ha superado todas mis expectativas. Desde el primer uso, noté una diferencia\n",
    "significativa en su rendimiento.\n",
    "Lo que más me gusta es su facilidad de uso, algo que no había encontrado en productos similares. Además, el servicio al cliente del fabricante es\n",
    "excepcional,\n",
    "aiempre dispuestos a ayudar. Sin duda, recomendaría este producto a todos mis amigos y familiares.\"\"\"\n",
    "review2 = \"\"\"\n",
    "Estoy absolutamente encantado con esta compra. El producto llegó en perfectas condiciones y mucho antes de lo esperado. Desde que lo estoy usando,\n",
    "mi vida ha cambiado para mejor.\n",
    "Es increíblemente eficiente y su diseño es elegante y moderno, lo cual es un gran plus. Aunque es un poco más caro que otros productos similares,\n",
    "vale totalmente la pena por la calidad que ofrece. Definitivamente, es una compra que volvería a hacer\n",
    "\"\"\"\n",
    "review3 = \"\"\"\n",
    "Este producto es, sin duda, uno de los mejores que he adquirido en años. Su calidad es indiscutible y los resultados son instantáneos. Lo que más\n",
    "me impresionó fue la tecnología\n",
    "innovadora que utiliza, algo que no he visto en ningún otro producto del mercado. Además, es extremadamente duradero y resistente. Aunque al\n",
    "principio estaba escéptico,\n",
    "ahora no puedo imaginar mi vida sin él. Lo recomiendo ampliamente a cualquiera que busque resultados de alta calidad\n",
    "\"\"\"\n",
    "review4 = \"\"\"\n",
    "Mi experiencia con este producto ha sido decepcionante. A pesar de las promesas del fabricante, no cumple con lo esperado. Desde el primer uso, he\n",
    "tenido problemas con su funcionamiento.\n",
    "Parece ser que vino roto tras el envío. Además, el servicio al cliente es poco útil y no responde a las consultas de manera eficiente.\n",
    "Francamente, me arrepiento de haber gastado mi dinero en esto y no lo recomendaría.\n",
    "\"\"\"\n",
    "review5 = \"\"\"\n",
    "Estoy realmente insatisfecho con este producto. Llegó con un defecto de fábrica y el proceso para obtener un reemplazo fue tedioso y frustrante.\n",
    "Incluso después de recibir uno nuevo, no funcionó según lo prometido. Su rendimiento es mediocre en comparación con otros productos similares que\n",
    "he probado.\n",
    "También es importante mencionar que la relación calidad-precio es muy baja. Definitivamente no es una compra que recomendaría a nadie\"\"\"\n",
    "\n",
    "reviews = ' \\n'.join([review1, review2, review3, review4, review5])\n",
    "prompt = f\"\"\"Tu tarea es extraer información releavante de las voloraciones de usuario \\\n",
    "para proporcionar feedback al departamento de logística. \\\n",
    "Analiza estas valoraciones, delimitadas por triples acentos diacríticos \\\n",
    "y elabora un feedbac de al menos 30 palabras, haciendo foco en aspectos que mencionan \\\n",
    "la logística del producto.\n",
    "Valoracoines: ```{reviews}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aquí tienes un snippet de código Python que utiliza la biblioteca Gradio para crear una interfaz que llama a una API y muestra imágenes en pantalla:\n",
      "\n",
      "```python\n",
      "import gradio as gr\n",
      "import requests\n",
      "from PIL import Image\n",
      "\n",
      "\n",
      "def get_image(url):\n",
      "    response = requests.get(url, stream=True)\n",
      "    response.raise_for_status()\n",
      "    image = Image.open(response.raw)\n",
      "    return image\n",
      "\n",
      "\n",
      "def predict_image(url):\n",
      "    try:\n",
      "        image = get_image(url)\n",
      "        return image\n",
      "    except Exception as e:\n",
      "        return str(e)\n",
      "\n",
      "\n",
      "input_text = gr.inputs.Textbox(label=\"URL de la imagen\")\n",
      "output_image = gr.outputs.Image()\n",
      "\n",
      "gr.Interface(fn=predict_image, inputs=input_text, outputs=output_image).launch()\n",
      "```\n",
      "\n",
      "Este código utiliza la función `get_image()` para obtener una imagen de una URL dada utilizando la biblioteca `requests` y luego utiliza la biblioteca `PIL` para abrir y retornar la imagen. La función `predict_image()` maneja cualquier error y retorna la imagen o un mensaje de error relevante.\n",
      "\n",
      "El código crea una interfaz de Gradio utilizando la función `gr.Interface()`, donde se especifica la función `predict_image()` como la función de predicción, el componente de entrada como un cuadro de texto para la URL de la imagen, y el componente de salida como una imagen. La función `launch()` inicia la interfaz y la muestra en el navegador.\n",
      "\n",
      "Recuerda reemplazar la URL de la API en la función `get_image()` con la URL real que estás utilizando.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Configurar el motor de OpenAI\n",
    "engine = \"gpt-3.5-turbo\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_completion(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=engine,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Eres un experto arquitecto de software, tu tarea es generar código de alta calidad, considerando las mejores prácticas de programación, con comentarios claros y un enfoque en la seguridad, eficiencia y la legibilidad.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "        ]\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "prompt = (\n",
    "    f\"Necesito un snippet de código en Python para crear una interfaz de gradio que permita llamar a una api y sacar imagenes por pantalla\"\n",
    ")\n",
    "respuesta = get_completion(prompt)\n",
    "print(respuesta.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_image(url):\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    image = Image.open(response.raw)\n",
    "    return image\n",
    "\n",
    "\n",
    "def predict_image(url):\n",
    "    try:\n",
    "        image = get_image(url)\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "def dummy_function(url):\n",
    "    # Replace this with your actual logic to process the input and generate output\n",
    "    return f\"Received URL: {url}\"\n",
    "\n",
    "input_text = gr.Textbox(label=\"URL de la imagen\")\n",
    "output_text = gr.Textbox()\n",
    "\n",
    "gr.Interface(fn=dummy_function, inputs=input_text, outputs=output_text).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/blocks.py\", line 1179, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/utils.py\", line 695, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/yc/q9_6ht1x43s552t2rfm6pw1c0000gn/T/ipykernel_53501/322019534.py\", line 18, in get_poster\n",
      "    return result_json['Poster']\n",
      "           ~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'Poster'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/blocks.py\", line 1179, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/utils.py\", line 695, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/yc/q9_6ht1x43s552t2rfm6pw1c0000gn/T/ipykernel_53501/322019534.py\", line 18, in get_poster\n",
      "    return result_json['Poster']\n",
      "           ~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'Poster'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/blocks.py\", line 1179, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/gustavo.aguado/anaconda3/envs/base_ml/lib/python3.12/site-packages/gradio/utils.py\", line 695, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/yc/q9_6ht1x43s552t2rfm6pw1c0000gn/T/ipykernel_53501/322019534.py\", line 18, in get_poster\n",
      "    return result_json['Poster']\n",
      "           ~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'Poster'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "url_omdb = \"http://www.omdbapi.com/?apikey=\" + os.getenv(\"OMDB_API_KEY\") + \"&t=\"\n",
    "def get_poster(movie):\n",
    "    \"\"\" \n",
    "    Devuelve la ruta al poster en jpg\n",
    "    \n",
    "    Args:\n",
    "        imdbId (str): El identificador IBMDB de la película\n",
    "    \"\"\"\n",
    "    movie = movie.replace(\" \", \"+\")\n",
    "    result = requests.get(url_omdb + movie)\n",
    "    result_json = result.json()\n",
    "    return result_json['Poster']\n",
    "\n",
    "def predict_image(url):\n",
    "    try:\n",
    "        image = get_image(url)\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "input_text = gr.Textbox(label=\"URL de la imagen\")\n",
    "output_image = gr.Image()\n",
    "\n",
    "gr.Interface(fn=get_poster, inputs=input_text, outputs=output_image).launch()\n",
    "#print(get_poster(movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
